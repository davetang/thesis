\section{History of DNA and genes}

In the winter of 1868/9, Swiss physician and biologist, Johannes Friedrich Miescher isolated an unknown substance from the nuclei of cells\cite{dahm2008discovering}. This substance was unlike anything he had observed before; it was resistant to protease, lacked sulphur, and contained a large amount of phosphorous. He recognised that he had isolated a novel substance and as it was from the nucleus, he named it nuclein. In 1881, Albrecht Kossel determined that nuclein was composed of five bases: adenine (A), cytosine (C), guanine (G), thymine (T), and uracil (U). Later in 1889, Richard Altmann discovered that nuclein was acidic (due to presence of phosphorous) and renamed nuclein as nucleic acid. The basic component of deoxyribonucleic acid (DNA) was deduced by Phoebus Levene in 1909, where he discovered that DNA consisted of an acid, an organic base, and a sugar; he also showed that these components were linked together as phosphate-sugar-base to form units, which he termed nucleotides. This sugar-phosphate backbone forms the structural framework of nucleic acids and makes DNA highly stable. In 1928, Frederick Griffith demonstrated that heritable traits could be transferred between dead and live bacteria and that provided the first clue that a ``transforming factor" existed\cite{griffith1928significance}. It wasn't until 1944, when Oswald Avery, Colin MacLeod, and Maclyn McCarty demonstrated that deoxyribonucleodepolymerase (an enzyme that degrades DNA) destroyed the ``transforming factor", that it was hypothesised DNA was the genetic material\cite{avery1944studies}. This was later confirmed in 1952 by Alfred Hershey and Martha Chase, by demonstrating that when bacteriophages infected bacteria, only their DNA would enter into the cytoplasm of the bacteria, while their protein remained outside\cite{hershey1952independent}.

While Phoebus Levene proposed that DNA was made up equal amounts of A, C, G, and T, it was later discovered by Erwin Chargaff that DNA should have a 1:1 ratio of pyrimidine (C, T, and U) and purine (A and G) bases\cite{pmid14938364, pmid14945441}; this became known as Chargaff's rules. The three-dimensional (3D) structure of DNA was solved by Francis Crick and James Watson in 1953\cite{WATSON_1953} after gaining insight from an X-ray diffraction image taken by Rosalind Franklin and from Chargaff's rules. The 3D structure of DNA showed how adenines paired with thymines and cytosines paired with guanine (Figure ~\ref{fig:dna}). The base pairing, now known as Watson-Crick base pairing, explained how genetic information could be copied.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=625,natheight=307]{dna.jpg}
   \caption[The structure of DNA]{The structure of DNA is based on the repeated pattern of a sugar and phosphate group, known as the sugar phosphate backbone, and the base pairing of the four bases, adenine (A), cytosine (C), guanine (G), and thymine (T). Image by NHS National Genetics and Genomics Education Centre licensed under the Creative Commons license.}
   \label{fig:dna}
\end{figure}

The definition of a gene has evolved with our increasing knowledge of genetics and biochemistry\cite{pmid17567988}. The idea of a gene dates back to Gregor Mendel when he demonstrated in his plant breeding experiments that discrete traits could be inherited from parents to offspring. While the mechanism of inheritance wasn't clear at that time, these heritable traits were termed a gene. The ``one gene, one polypeptide" hypothesis was the idea that each gene was responsible for producing a single enzyme/protein in a biochemical pathway; this was formed after observing that mutations in Neurospora genes would cause defects in steps of a metabolic pathways\cite{pmid16578042}. The relationship between DNA and proteins was demonstrated by the use of artificial ribonucleic acid (RNA) and bacterial cells\cite{pmid14471390}. RNA is synthesised by an RNA polymerase, using DNA as a template, in a process known as transcription. By using RNA artificially created to be entirely composed of uracils, Matthaei and colleagues produced a protein entirely composed of the amino acid phenylalanine. This experiment demonstrated that nucleic acids contained a code for amino acids. This code, known as the genetic code, was later cracked three years later\cite{pmid5330357} and defined how information is encoded in the genetic material. Nirenberg and colleauges worked out that three nucleotides defined a codon and is translated into one of the 20 standard amino acids. This flow of information from nucleic acid to protein was described as the ``Central Dogma"\cite{crick1958protein}.

\section{DNA to RNA}

The genome is a store of biological information that requires the coordinated activity of enzymes and proteins to bring it to life. This is achieved via transcription, which is a highly regulated mechanism that begins with RNA polymerase attaching to DNA and processes the template DNA strand into a single-stranded RNA molecule (Figure ~\ref{fig:transcription}). Transcription results in two main classes of RNA or transcripts: (1) Protein-coding transcripts, where the RNA known as messenger RNA (mRNA) can be further translated into a protein molecule and (2) Non-coding transcripts, where the RNA molecule is the functional product. There are three different types of RNA polymerases in eukaryotic cells: Pol I transcribes the genes that encode most of the ribosomal RNAs (rRNAs); Pol II transcribes the mRNAs; and Pol III transcribes the genes for small regulatory RNA molecules, such as transfer RNA (tRNA). RNA polymerase binds upstream to the DNA that will be transcribed at a region known as the promoter. Promoters can be classified by their distance to the transcription start site (TSS), which are the first nucleotides transcribed by RNA polymerase. The core promoter is the closest to the TSS and contains specific DNA sequences or elements that are necessary for transcription. The core promoter elements include the TATA box (usually located 25 to 35 bases upstream of the TSS), the initiator element (Inr), the downstream promoter element (DPE), the TFIIB recognition element (BRE) and the CpG island (CGI). The proximal promoter is the next closest (at $\sim~250$ bp) and usually contains primary regulatory elements; distal promoters are even further upstream and contain additional regulatory elements. Transcription with Pol I and Pol III are similar, but the promoter sequences and activator proteins differ.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=842,natheight=595]{transcription.jpg}
   \caption[DNA transcription]{The process of transcription begins with RNA polymerase, which reads the DNA sequence (the template strand) in the 5' to 3' direction and produces a complementary RNA strand called the messenger RNA (mRNA). The mRNA has the same sequence as the coding strand, except that thymines are replaced by uracils. Image by NHS National Genetics and Genomics Education Centre licensed under the Creative Commons license.}
   \label{fig:transcription}
\end{figure}

\subsection{Transcription factors}

Transcription factors (TFs) are regulatory proteins that can activate or enhance the transcription of DNA by binding to specific DNA sequences at promoters (they are also known to negatively regulate transcription but this is less common). TFs contain one or more DNA-binding domains that allow it to bind to specific sequences. For example, the TATA binding protein (TBP), known as a general transcription factor (GTF), binds to the TATA box and is involved in DNA strand separation during transcription. Other GTFs include TFIIA, TFIIB, TFIID, TFIIE, TFIIF, and TFIIH, and are necessary for transcription to take place. Not all the GTFs bind to DNA but are part of the large transcription preinitiation complex (PIC) that interacts with RNA polymerase and helps position RNA polymerase over the TSS. As transcription takes place, the DNA double helix unwinds and RNA polymerase reads the template strand; the RNA sequence will have the same sequence as the coding strand. While the rate of transcription directly governs the number of transcripts that are present, the stability of transcripts also play a part.

\subsection{Post-transcriptional modifications}

The termination of transcription relies on terminator sequences that are found close to the ends of transcripts. For Pol I transcripts, transcription is stopped by a termination factor that unwinds the DNA-RNA hybrid formed during transcription. For Pol III transcripts, inverted repeats are transcribed and these sequences can form hairpin loops that pauses the RNA polymerase. For Pol II transcripts, a polyadenylation signal (AAUAAA) at the end of the sequence is necessary for termination\cite{pmid3479794}. The nascent RNA is cleaved at the polyadenylation site and a poly-adenine tail (poly(A)) is added; the poly(A) tail is important for the stability of the RNA, translation, and for nuclear export. In addition to this, a specialised nucleotide cap is added to the 5' end and this is the site recognised by the ribosomes in protein synthesis. The ribosome binds to the cap and reads along the 5' untranslated region (UTR) until it reaches an initiation codon. Removal of the cap is considered to the first step towards mRNA degradation. The capping procedure is thought to occur only in the nucleus, however a cytoplasmic form of a capping enzyme has been identified\cite{pmid22921400}. The default state of transcription in eukaryotes is that most genes are not constantly transcribed, unlike in prokaryotes. Structural properties of DNA make it inaccessible to the PIC; chromatin structure and nucleosome positioning are altered in order for the transcriptional machinery to access parts of the genome for transcription.

\section{DNA packaging}

Inside every somatic nucleated cell in the human body are 23 pairs of chromosomes; these chromosomes make up the human genome. A chromosome is a long DNA molecule that is condensed so that it may physically fit inside the nucleus (Figure ~\ref{fig:dna_condensed}). We can estimate the physical length the human genome by multiplying the number of base pairs contained on each chromosome by the length of a base pair. The haploid human genome contains around 3\e{9} base pairs of DNA; therefore there is a total of 6 billion base pairs of DNA per cell. Given that each base pair of DNA is about 0.34 nanometers long or 3.4\e{-10} meters\cite{pmid7354864}, each diploid cell contains 3.4\e{-10} * 3\e{9} * 2 or 2.04 meters of DNA. Given that the estimated number of cells in the human body is around 3.7\e{13} or 37 trillion\cite{pmid23829164}, a typical person contain around 74 trillion meters of DNA. Certain proteins, called histones, help compact the vast amounts of DNA inside of us (specifically, inside the eukaryotic nucleus). The histones are a family of small, positively charged proteins that provide the energy, in the form of electrostatic interactions, to fold negatively charged DNA (due to the phosphate groups in its phosphate-sugar backbone). The core histones are H2A, H2B, H3, and H4 and the resulting DNA-protein complex is called chromatin.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=650,natheight=539]{dna_condensed.jpg}
   \caption[Condensation of DNA]{DNA is condensed into chromosomes by forming DNA-protein complexes known chromatin, which is further coiled into thicker fibers called 30nm fibers. The chromosomes reside inside the nucleus of a cell. Mitochondria also contain its own DNA. Image by NHS National Genetics and Genomics Education Centre licensed under the Creative Commons license.}
   \label{fig:dna_condensed}
\end{figure}

Chromatin possesses a fundamental repeating structure\cite{holde01111974}, known as nucleosomes and the packaging of DNA into nucleosomes shortens DNA about sevenfold. However despite this, chromatin is still too long to fit inside the nucleus, which is approximately 10 to 20 microns in diameter. Chromatin is further coiled into a thicker fiber, called the 30nm fiber, because it is roughly 30 nanometers in diameter. Processes such as transcription and replication require the two strands of DNA to come apart temporarily, thus allowing polymerases access to the DNA template. However, the presence of nucleosomes and the folding of chromatin into 30-nanometer fibers pose barriers to the enzymes that unwind and copy DNA. It is therefore important for cells to have means of opening up chromatin fibers and/or removing histones transiently to permit transcription and replication to proceed.

\subsection{Histone modifications}

Generally speaking, there are two major mechanisms by which chromatin is made more accessible:

\begin{enumerate}
   \item Histones can be enzymatically modified by the addition of acetyl, methyl, or phosphate groups.
   \item Histones can be displaced by chromatin remodelling complexes, thereby exposing underlying DNA sequences to polymerases and other enzymes.
\end{enumerate}

These two processes are reversible, so modified or remodelled chromatin can be returned to its compact state after transcription and/or replication are complete. The common nomenclature of histone modifications is by the name of the histone (e.g. H3), the single-letter amino acid abbreviation (e.g. K for lysine) and its position (e.g. 27), and the type of enzymatic modification (e.g. acetylation); put together this is represented as H3K27ac. Specific histone modifications are linked to different biological states; for example, acetylation removes the positive charge on the histones, decreasing the interaction between histones and DNA, leading to transcriptional activation. On the other hand, the tri-methylation of lysine 27 on histone H3, i.e., H3K27me3, is associated with the inhibition of transcription\cite{pmid21652639}. Given that distinct histone modifications can either activate or repress transcription, a ``histone code" has been proposed\cite{pmid11498575} and the profiling of the histone states will provide clues to the transcriptional state of a DNA region.

\section{DNA sequencing}

The process of DNA sequencing is the determination of the exact order of nucleotides within a DNA molecule. The first generation of DNA sequencing methods consisted of the electrophoretic methods (Sanger and Maxam-Gilbert sequencing), which were very labour intensive, requiring 4 separate reactions for the determination of each base. One of the first innovations was the use of different fluorophores with Sanger sequencing, which allowed the reactions to be pooled together during gel electrophoresis and eliminated the use of radioactive material. Automation of the DNA sequencing process was possible with the development of an apparatus that could detect the fluorescence emitted by the chain terminated fragments. This development was key towards the success of the Human Genome Project (HGP). For over 25 years since its inception, Sanger sequencing was the method of choice for DNA sequencing.

\subsection{Sanger and Maxam-Gilbert sequencing}

Sanger sequencing\cite{pmid271968} and Maxam-Gilbert sequencing\cite{pmid265521} were two methods of DNA sequencing developed in the 1970s requiring the use polyacrylamide gel electrophoresis, which allowed the resolution of DNA fragments at a 1 bp resolution, and allowed the determination of the DNA bases. The key feature of Sanger sequencing is the use of dideoxynucleotide triphosphates (ddNTPs) and a purified DNA polymerase enzyme to synthesise DNA. The structure of a normal nucleotide (dNTP), consists of a 3' hydroxyl (OH) group in the pentose sugar. The chain-terminating ddNTPs lack the OH group that is necessary for the formation of the phosphodiester bond between one nucleotide and the next during DNA strand elongation. If a ddNTP is incorporated into a growing DNA strand, the strand elongation is terminated. The idea is to set up a reaction with a mixture of dNTPs [deoxyadenosine triphosphate (dATP), deoxyguanosine triphosphate (dGTP), deoxycytidine triphosphate (dCTP), deoxythymidine triphosphate (dTTP)] and one particular ddNTP in a ratio of 300:1. Most of the times, the DNA will be elongated but once the ddNTP is incorporated the strand stops. This results in a number of DNA fragments of varying lengths and depending on the ddNTP used, the last base of the fragment corresponds to that base. This reaction is carried out for the other ddNTPs and all the fragments are separated using polyacrylamide gel electrophoresis. By reading the ladder, the DNA bases can be deduced. The Maxam-Gilbert sequencing method relies on the use of chemicals that can cleave specific bases. Dimethyl sulfate is used to cleave purine bases (A and G) and hydrazine is used to cleave pyrimidine bases (C and T). To distinguish the purines, an adenine-enhanced cleavage step is carried out, which cleaves adenines preferentially. To distinguish the pyrimidines, NaCl is used with hydrazine to suppress the reaction of thymines. As with Sanger sequencing, the DNA fragments are separated using polyacrylamide gel electrophoresis, and the DNA bases are deduced by reading the gel.

Sanger sequencing became the \textit{de facto} method for DNA sequencing due to its comparative ease and the use of fewer toxic materials than Maxam-Gilbert sequencing. A further improvement to Sanger sequencing replaced the need to radioactively label the DNA fragments by using chemically synthesised fluorescent oligonucleotide primers\cite{pmid3713851}. Four different fluorophores were used for each ddNTP reaction allowing all four reactions to be co-electrophoresed and the DNA sequence was deduced by reading the fluorescence colours. The development of a fluorescence detection apparatus linked to a computer that processed the data created the world's first partially automated DNA sequencer\cite{pmid3713851}.

\subsection{Next-generation sequencing}

The next wave of DNA sequencing methods, the so-called next-generation (next-gen) or second generation sequencing, started with various strategies that rely on a combination template preparation, sequencing, and imaging that allowed thousands to billions of sequencing reactions to be performed simultaneously\cite{pmid19997069}. Next-gen sequencing relies on the clonal amplification of templates and use \textit{in vitro} cloning rather than bacterial cloning; the two most common methods are emulsion PCR (emPCR)\cite{pmid12857956} and solid-phase amplification\cite{pmid16473845}. With emPCR individual DNA molecules are isolated with primer-coated beads in water-in-oil microreactors and clonal amplification leads to thousands of copies of the DNA molecule in an emulsion. 454 pyrosequencing and SOLiD sequencing use emPCR, where the amplification product is deposited into individual wells for sequencing. Solid-phase amplification relies on a lawn of high-density primers that are covalently attached on a slide surface (also known as a flow cell) and bind to DNA molecules ligated with sequencing adaptors. The two methods allow each DNA template to be spatially separated, allowing massively parallel sequencing to take place.

Sequencing can take place via the use of DNA polymerase, which is commonly known as sequencing-by-synthesis, and via the use of DNA ligase, known as sequencing-by-ligation (SBL). SBS can be further classified into cyclic reversible termination (CRT), single-nucleotide addition (SNA) and real-time sequencing\cite{pmid19997069}. CRT uses reversible terminators and initial developments used the same dideoxynucleotides used as chain terminators in Sanger sequencing. The concept of CRT is that a DNA polymerase incorporates one fluorescently modified nucleotide, which has a reversible terminator that terminates DNA synthesis. Unincorporated nucleotides are washed away and fluorescence imaging takes place to determine the identity of the incorporated nucleotide. The last step removes or cleaves the reversible terminator and the fluorescent dye and the cycle is repeated. The CRT method is used in Solexa/Illumina and Helicos sequencing. SBL relies on DNA ligase and either one-base-encoded or two-base-encoded probes that are fluorescently labelled. The probes hybridise to its complementary sequence on the primed template and DNA ligase is added to join the probe to the primer. Non-ligated probes are washed away followed by fluorescence imaging and cleavage of the fluorescent dye and the cycle is repeated. The SBL method is used in SOLiD sequencing.

\subsection{Third generation sequencing and beyond}

The third generation of sequencing refers to single-molecule sequencing technologies, which have the capacity for longer read lengths at potentially cheaper costs\cite{pmid20858600}. One of the main advantages of single-molecule sequencing is that PCR is not required, therefore amplification biases and PCR mutations are eliminated. Furthermore, quantitative applications of sequencing such as RNA sequencing are much more representative of the true abundance of RNA molecules. The HeliScope sequencer was the first commercially available single-molecule sequencer, which was based on the work of Stephen Quake and colleagues\cite{pmid12651960}. HeliScope sequencing utilises billions of primed single-molecule templates are covalently attached to the solid support and uses CRT but with slight differences from Solexa/Illumina sequencing. HeliScope uses Helicos Virtual Terminators, which differ from the reversible terminators used in Solexa/Illumina sequencing and dye labelled nucleotides are added individually in the predetermined order of C, T, A, and G, which is followed by fluorescence imaging.

With the advent of next-gen sequencing we have the capacity to sequence an entire human genome in a matter of days. Developments in next-gen sequencing are aiming towards longer read lengths with higher output (Figure ~\ref{fig:dev_next_gen}) and we have just recently arrived in the \$1,000 genome era, whereby we can sequence the entire genome of an individual for around \$1,000 US dollars (USD). In contrast, the Human Genome Project (HGP), which gave us the first glimpse of the human genome\cite{lander2001initial} costed approximately 2.7 billion fiscal year 1991 US dollars\cite{nhgri2010cost}. Next-gen sequencing is also not just limited to genome sequencing; reverse transcriptase\cite{pmid4316300, pmid4316301} allow for the synthesis of complementary DNA (cDNA) from RNA and therefore the entire collection of RNA, known as the transcriptome, can be sequenced. Other applications of next-gen sequencing has allowed us to capture in a genome-wide manner DNA-protein interactions, DNA methylation patterns, and histone modifications\cite{applicationsofsequencing} and the number of protocols utilising next-gen sequencing are continually growing\cite{pachter2014seq}.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=720,natheight=540]{developments-in-next-generation-sequencing.jpg}
   \caption[Developments in next generation sequencing]{Log read length versus log gigabases per run for various next generation sequences\cite{Nederbragt2012}.}
   \label{fig:dev_next_gen}
\end{figure}

\section{RNA sequencing}

The transcriptome is the complete collection of RNA molecules in a cell at a specific developmental stage or under a specific physiological condition. The profiling of transcripts or expression profiling reveals the molecular constituents (the RNA and the promoter) that are active under a specific condition and allows the inference of biological pathways and regulatory mechanisms. One of the first technologies that allowed the simultaneous profiling of thousands of transcripts was microarrays\cite{pmid7569999}, which is a hybridization-based approach. Basically, probes that are complementary to specific DNA sequences, such as protein-coding transcripts or genomic regions, are attached to a solid surface. One of the first studies using microarrays showed that the switching from aerobic to anaerobic respiration in yeast changed the levels of over 700 mRNAs by two fold or higher\cite{pmid9381177}. However microarrays have several limitations, which includes \textit{a priori} knowledge of the genome or transcript sequences in order for the probes to be designed, high background levels from cross-hybridisation\cite{pmid16749918}, and a limited dynamic range in expression, which relies on fluorescence signal. 

RNA sequencing (RNA-Seq) is an application of next-gen sequencing used to catalogue the transcriptome as well as quantifying the expression levels of individual transcripts\cite{pmid19015660}. RNA-Seq requires no \textit{a priori} knowledge of the genome, provides a digital gene expression (DGE) profile, and does not suffer from cross-hybridisation issues, which are all limitations of microarrays. RNA-Seq can be applied to total RNA libraries, which sequences the entire population of RNA, or poly-A libraries, which sequences transcripts that have a poly-A tail (mostly mRNAs). A variant of RNA-Seq is small RNA sequencing, which uses size selection to purify RNA that are less than 200 nucleotides, followed by adaptor ligation and sequencing. The sequencing of larger RNA molecules requires an additional fragmentation step, which can introduce biases\cite{pmid18516045}. While RNA-Seq encapsulates all RNA sequencing applications, it is usually associated with the fragmentation of RNA followed by sequencing. There are some protocols that sequence RNA but only the 5'\cite{pmid15300261,pmid14663149} or 3'\cite{pmid22454233} ends of RNA.

\subsection{Cap Analysis Gene Expression}

Cap Analysis Gene Expression (CAGE) was initially conceived as an idea of profiling all active promoters and TFs for understanding the interplay between the two\cite{carninci2010capanalysis}. CAGE uses a molecular technique known as Cap-Trapper\cite{pmid8938445,pmid9179497}, which allows the capturing of all RNAs that have a cap structure (specifically the diol group of RNA were biotinylated). (Figure ~\ref{fig:cage_protocol}). CAGE allows the genome-wide identification of transcription start sites as well as their expression levels and has been used in various FANTOM projects\cite{pmid16141072, pmid19377474, pmid24670764} and in ENCODE\cite{pmid17571346, pmid22955620}. In the FANTOM3 project, CAGE unveiled the transcriptional landscape in mammalian genomes\cite{pmid16141072} and identified many novel mRNAs and non-coding RNAs not previously characterised and revealed two major classes of promoters: conserved TATA box-enriched promoters and CpG-rich promoters\cite{pmid16645617}. Two variants of CAGE include nanoCAGE\cite{pmid20543846} and HeliScopeCAGE\cite{pmid21596820}. NanoCAGE utilises the template-switching method\cite{pmid11314272} instead of Cap-Trapper and allows for smaller starting amounts of RNA, to the level of RNA content in single cells. HeliScopeCAGE utilises Cap-Trapper but without the enzymatic tag cleavage and PCR amplification, and the capped 5' ends of the cDNA are directly sequenced on the HeliScope sequencer.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=2254,natheight=3051,totalheight=0.5\textheight]{cage_protocol.png}
   \caption[Cap Analysis Gene Expression protocol]{The Cap Analysis Gene Expression (CAGE) protocol starts with synthesising cDNA from either total RNA or mRNA by using random or oligo dT primers (only random primers are shown here). Reverse transcription takes place in RNAs with or without a cap and to full or partial completion; the RNase I digestion removes partially reverse transcribed RNA as they are not protected by a full double strand. The 5' end of cDNAs are selected by streptavidin beads and unbound cDNA are washed out. After release from the bead, a linker is attached to the 5' end of the single-stranded cDNA; this linker contains recognition sites that allow the endonuclease cleavage. Lastly a linker is attached to the 3' end of the tag sequence, which is amplified and directly sequenced.}
   \label{fig:cage_protocol}
\end{figure}

\subsection{Transcriptome studies}

SAGE\cite{pmid9008165}.

RNA-Seq offers an unbiased view of the transcriptome since it does not rely on any genomic annotations, which has led to the discovery of previously non-annotated transcripts.

Tiling arrays and full length cDNA sequencing suggested that most of the genome is transcribed


Catalogue of transcribed sequences such as the collection of mouse full length cDNAs by the FANTOM consortium revealed many transcripts of unknown function (TUFs)\cite{pmid16141072}. 38.6\% of the FANTOM3 mouse full-length cDNAs have very low coding potential (CPAT coding probability of less than 0.2) (Figure ~\ref{fig:fantom3_coding_prob})

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=1000,natheight=718]{fantom3_coding_probability.png}
   \caption[Coding probability of FANTOM3 mouse cDNAs]{Distribution of coding probabilities of FANTOM3 mouse full-length cDNAs as predicted by CPAT\cite{tang2014fantom3codingprob}.}
   \label{fig:fantom3_coding_prob}
\end{figure}

\subsection{Non-coding RNAs}

Diverse class of non-coding RNAs, broadly broken down into long and short non-coding RNA

Classical non-coding RNA found in both prokaryotes and eukaryotes are ribosomal RNAs and transfer RNAs, which are both involved in protein synthesis

Micro-RNAs (miRNAs) were first first observed in 1993 in Caenorhabditis elegans\cite{pmid8252621} as a small double-stranded RNA that were bound to the 3' untranslated region (UTR) of an mRNA. The binding of the miRNA inhibited the translation of the mRNA and is one of the mechanisms by which miRNAs post-transcriptionally regulate gene expression. The biogenesis of miRNAs begins with the cleavage by an enzyme called Dicer\cite{pmid11201747}, which is part of the RNase III family. miRNAs inhibit translation by recruiting a ribonucleoprotein complex called RNA-induced silencing complex (RISC). Short-interfering RNAs are also cleaved by Dicer and can also bind to mRNA but require complete complementarity. MiRNAs are products of dsRNAs encoded in genes in our genome and do not require full complementarity to bind to a target mRNA thus one miRNA may regulate several genes.

A single-stranded nucleic acid molecule will tend to fold up on itself to form localised double-stranded regions, producing structures called hairpins or stem-loop structures. This is due to the hydrophobic nature of the bases, which means that the bases are unstable when exposed to an aqueous environment. Pairing of the bases enables them to be removed from interaction with the surrounding water and stabilising the DNA helix.

Assembly of non-coding RNAs with proteins as ribonucleoprotein (RNP) structures
Interaction of non-coding RNAs with chromatin

\section{The repetitive genome}

The FANTOM and ENCODE projects demonstrated that a large percentage of the mammalian genome is transcribed, which became known as pervasive transcription\cite{pmid21765801}. Critique of ENCODE\cite{pmid23431001, pmid23479647, pmid23137679}.

Since the release of the draft human genome sequence\cite{venter2001sequence, lander2001initial}, it was established that only a small fraction of the genome is made up of protein-coding sequences and the majority of the genome was made up with repetitive elements. As the human genome contains the entire instruction set that is necessary for the development of a human, the decoding of the genome was thought to provide answers to many outstanding biological questions. However, there are several paradoxes that are still currently unresolved:

\begin{enumerate}
   \item K-value paradox: complexity does not correlate with the number of chromosomes
   \item C-value paradox: complexity does not correlate with genome size
   \item N-value paradox: complexity does not correlate with the number of protein coding genes
\end{enumerate}

If we measure organismal complexity in terms of mental cognition, we observe that complexity is not correlated to the number of chromosomes, the size of genomes, and the number of protein coding genes. Humans have 46 chromosomes and some species of butterflies have over hundreds of chromosomes, such as \textit{Polyommatus atlantica}. In terms of genome size, the species \textit{Polychaos dubium}, a freshwater amoeboid, has one of the largest genomes known with around 670 gb of DNA sequence; humans on the other hand have a genome size of roughly 3 gb. And lastly humans have roughly the same number of protein-coding genes as \textit{Caenorhabditis elegans}, roughly 20,000 versus 21,000, respectively. However, the \textit{C. elegans} genome is about 100 mb\cite{celegans1998sequencing}, which is approximately 30 times smaller than the human genome, despite having a similar number of genes. One of the discrepancies between genome sizes despite having a similar number of genes is due to repetitive elements, which makes up roughly 50\% of the human genome. Among 66 vertebrate genomes, the percentage coverage of repetitive elements in the human genome is quite high (Figure ~\ref{fig:repeat_coverage_vertebrate_genome}).

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=1600,natheight=1148]{repeat_coverage_vertebrate_genome.png}
   \caption[Coverage of repetitive elements in vertebrate genomes]{The total coverage of repetitive element in 66 vertebrate genomes as annotated by RepeatMasker in the respective genomes\cite{tang2014repcoverage}.}
   \label{fig:repeat_coverage_vertebrate_genome}
\end{figure}

However, the larger vertebrate genomes do not always contain the highest percentage of repetitive elements (Figure ~\ref{fig:genome_size}). At least in humans, transposons make up the majority of the repetitive elements that make up the human genome. In particular class I transposons (retrotransposons), which are able to transcribed and inserted into the genome, make up a large portion of the human genome. W.Ford Doolittle and Carmen Sapienza wrote in 1980\cite{doolittle1980selfish}: ``When a given DNA, or class of DNAs, of unproven phenotypic function can be shown to have evolved a strategy (such as transposition) which ensures its genomic survival, then no other explanation for its existence is necessary" and Leslie Orgel and Francis Crick, wrote that junk DNA has little specificity and conveys little or no selective advantage to the organism\cite{orgel1980selfish}.

The origin of the term ``junk DNA" is usually attributed to Susumu Ohno, who used it to describe pseudogenes, which are gene copies that have no known biological function. In its modern day usage, ``junk DNA" is used to describe DNA sequence that goes not play a functional role in an organism. Dr. Ohno estimated that there would be an upper limit to the number of functional loci in mammalian genomes based on mutational load and a fixed mutation rate. He predicted that mammalian genomes could not have more than 30,000 loci under selection as this would guarantee a progressive decline in fitness, leading to extinction.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=1600,natheight=932]{genome_size.png}
   \caption[Vertebrate genomes sizes]{The genome sizes of 66 vertebrate genomes, sorted from the lowest to highest percent of repetitive element coverage\cite{tang2014gensize}.}
   \label{fig:genome_size}
\end{figure}

\begin{itemize}
   \item Repeat-associated binding sites (RABS) are over-represented in proximity of regulated genes and that the binding motifs within these repeats have undergone evolutionary selection
   \item Indeed, studies conducted both at the gene and genome levels have uncovered TE insertions that seem to have been co-opted - or exapted - by providing transcription factor binding sites (TFBSs) that serve as promoters and enhancers, leading to the hypothesis that TE exaptation is a major factor in the evolution of gene regulation.
   \item The transcription of repetitive elements, especially transposable elements, in specific tissues
   \item Repetitive elements are usually highly methylated, however differentiation methylation patterns of repetitive elements was observed in specific tissues
   \item Functions of some GWAS candidates in intergenic regions (such as http://www.nature.com/nature/journal/v507/n7492/full/nature13138.html)
   \item The regulated retrotransposon transcriptome of mammalian cells\cite{pmid19377475}
\end{itemize}

\section{Stem cells}

The existence of stem cells was discovered in 1961 and the stem cell theory was published in 1963\cite{pmid13970094}. In this work, Canadians researchers James Till and Ernest McCulloch proved that stem cells were present in bone marrow. They exposed mice to high doses of radiation that killed off the mouse's blood and immune forming system and then injected bone marrow cells into some of the mice. The mice that didn't receive the transplants died and the mice that received the transplants survived because the bone marrow cells rebuilt their blood and immune forming systems. The bone marrow cells were able to reproduce themselves as well as generating different cell types. Human bone marrow transplants are still routinely used nowadays to treat leukaemia and other kinds of blood disorders.

Stem cells have two key characteristics that differ from other cells: they can reproduce themselves for long periods of time, known as self-renewal, and they can differentiate or specialise into specific cell types under certain conditions. What Till and McCulloch discovered were actually adult or tissue stem cells (ASCs), which are multipotent; this means that they have limited differentiation ability and are only able to generate specific types of differentiated cells. For example, haematopoietic stem cells, which are a type of adult stem cells that reside in the medulla of the bone (bone marrow), are only able to give rise to different mature blood cell types and tissues. In 1981, researchers were able to derived embryonic stem cells (ESCs) from mouse embryos (specifically the inner cell mass of blastocysts) and grow them on Petri dishes\cite{pmid7242681, pmid6950406}. In normal development, the inner cell mass begins growing into all the different cell types of the fully developed body and as such ESCs are able to differentiate into all the cell types that make up the body and they are pluripotent. Cells with the highest potency or with the most differentiation potential, are known as totipotent cells and are able to form all the cell types in a body, including the placental cells. Embryonic cells within the first couple of cell divisions after fertilisation are the only cells that are totipotent.

In 2006, a new methodology was invented by Takahashi and Yamanaka to generate another type of stem cell, called induced pluripotent stem cells (iPSCs)\cite{pmid16904174}. In their work, they discovered a way to generate pluripotent stem cells from somatic cells, which have no differentiation potential. Since their discovery, many researchers wondered about the development potential of iPSCs and in 2009 a Chinese group were successful in creating a live mice from iPSCs that were able to produce offspring\cite{pmid19672241}. In one of the first studies to show the clinical application of human iPSCs to human disease, a team derived iPSCs from fibroblasts of patients suffering from alpha 1 antitrypsin deficiency, corrected the disease causing mutation, and used the corrected stem cells to produce liver cells that were transplanted into the liver via intrasplenic injection\cite{pmid21993621}. Since the cells are genetically identical to the patient, there is no worry about rejection or immune problems, and is one reason why iPSCs are an attractive option for regenerative medicine.

\section{The role of bioinformatics in genomics}

Modern day high-throughput sequencers generate large amounts of data; for example in the blood transcriptome project (Chapter 7), one lane of sequencing on the HiSeq2000 produced 74.5 million reads (using 15 gigabytes of storage space when uncompressed). To deal with data at this scale, informatics tools for storing, managing, and analysis such data sets are absolutely necessary. Bioinformatics can be thought of as informatics for biological data, though historically it was defined as ``the study of informatic processes in biotic systems"\cite{pmid21483479}. The HGP was one of the first large scale international research efforts and bioinformatics was crucial towards the successful completion of the HGP\cite{stein1996perl}. An important principle established during the HGP, called the ``Bermuda Principles", was set by an international assortment of genome-research leaders towards the rapid and public sharing of human genome information. These set of commitments left a lasting legacy in large genomic science projects such as The International HapMap Project, ENCODE and modENCODE, and The Cancer Genome Atlas where data was made freely available prior to publication\cite{contreras2011bermuda}. The public availability of data from these projects have resulted in major advances in genomics and bioinformatics.

The FASTQ format was formally defined in 2010\cite{pmid20015970} and has become the \textit{de facto} format for storing raw sequencing output. FASTQ is similar to the FASTA format with the addition of storing quality scores for each nucleotide. The format is simplistic, which may be the reason for its popularity before being formalised. The establishment of a standard format is important as developers can produce programs, such as sequence aligners, that can process output from different sequencing machines. Another standard file format for storing sequence alignments is the Sequence Alignment/Map (SAM) format\cite{pmid19505943}.

The reads generated from the second generation of sequencers were much shorter and much more numerous than traditional Sanger sequencing reads. The Illumina GAII can produce up to 100 million reads of 50 bp in a single run. Traditional sequence alignment tools using the Needleman–Wunsch algorithm or Smith–Waterman algorithm were simply too slow and new developments were necessary. The popular short-read alignment tool, BWA\cite{pmid19451168}, implements the Burrows–Wheeler transform to deal with the vast number of short reads.

Comparing genomic features using BEDTools\cite{pmid20110278}.

Expression data sets are commonly stored as matices; for example if we let $A$ be an $m \times n$ matrix, where $a_{ij}$ are elements of $A$, the $i^{th}$ row would represent the transcriptional response of the $i^{th}$ gene and the $j^{th}$ column would represent the expression profile of the $j^{th}$ assay:

\begin{align*}
   A = \begin{bmatrix} a_{11} & \cdots & a_{1j} & \cdots & a_{1n} \\
   . && . && . \\
   a_{i1} & \cdots & a_{ij} & \cdots & a_{in} \\
   . && . && . \\
   a_{m1} & \cdots & a_{mj} & \cdots & a_{mn} \end{bmatrix}
\end{align*}

A differential expression analysis can be easily performed on an expression matrix, such as by using the edgeR package\cite{pmid19910308} from Bioconductor\cite{pmid15461798}. Multidimensional scaling methods, such as Singular Vector Decomposition (SVD), can also be applied on the matrix to find underlying patterns in the data. A very popular visualisation method of expression data is the use of heatmaps that reflect the relative expression strength of each element in the matrix ($a_{mn}$), which is commonly used in tandem with hierarchical clustering. Co-expression matrices can be further derived from the expression matrix to find transcripts with a similar transcriptional response or assays with similar expression profiles. Networks or graphs can be built from such co-expression matrices for visual purposes or for analysing the structure of co-expression patterns.

