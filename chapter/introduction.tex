\section{A brief history of DNA}

In the winter of 1868/9, Swiss physician and biologist, Johannes Friedrich Miescher isolated an unknown substance from the nuclei of cells\cite{dahm2008discovering}. This substance was unlike anything he had observed before; it was resistant to protease, lacked sulphur, and contained a large amount of phosphorous. He recognised that he had isolated a novel substance and as it was from the nucleus, he named it nuclein. In 1881, Albrecht Kossel determined that nuclein was composed of five bases: adenine (A), cytosine (C), guanine (G), thymine (T), and uracil (U). Later in 1889, Richard Altmann discovered that nuclein was acidic (due to the presence of phosphorous) and renamed nuclein to nucleic acid. The basic component of deoxyribonucleic acid (DNA) was deduced by Phoebus Levene in 1909, where he discovered that DNA consisted of an acid, an organic base, and a sugar. Levene also showed that these components were linked together as phosphate-sugar-base to form units, which he termed nucleotides. This sugar-phosphate backbone forms the structural framework of nucleic acids and makes DNA highly stable. In 1928, Frederick Griffith demonstrated that heritable traits could be transferred between dead and live bacteria and that provided the first clue that a ``transforming factor" existed\cite{griffith1928significance}. It wasn't until 1944, when Oswald Avery, Colin MacLeod, and Maclyn McCarty demonstrated that deoxyribonucleo-depolymerase (an enzyme that degrades DNA) destroyed the ``transforming factor", that it was hypothesised DNA was the genetic material\cite{avery1944studies}. This was later confirmed in 1952 by Alfred Hershey and Martha Chase, by demonstrating that when bacteriophages infected bacteria, only their DNA would enter into the cytoplasm of the bacteria, while their protein remained outside\cite{hershey1952independent}.

While Levene proposed that DNA was made up of equal amounts of A, C, G, and T, it was later discovered by Erwin Chargaff that DNA had a one-to-one ratio of pyrimidine (C, T, and U) and purine (A and G) bases\cite{pmid14938364, pmid14945441}; this became known as Chargaff's rules. This observation by Chargaff and insights gained from Rosalind Franklin were necessary for the deduction of the three-dimensional (3D) structure of DNA by Francis Crick and James Watson in 1953\cite{WATSON_1953}. The 3D structure of DNA demonstrated how adenines paired with thymines and cytosines paired with guanine (Figure ~\ref{fig:dna}); this became known as Watson-Crick base pairing and explained how genetic information could be copied due to the complementary nature of DNA.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth, natwidth=626, natheight=579, totalheight=0.40\textheight, keepaspectratio]{dna_scitable.jpg}
   \caption[DNA base pairing]{The structure of DNA is based on the repeated pattern of deoxyriboses and phosphate groups, forming the sugar-phosphate backbone, and the base pairing of the four bases, adenine (A), cytosine (C), guanine (G), and thymine (T). Two hydrogen bonds connect A to T and three hydrogen bonds connect C to G. Image used with permission from Nature Education 2013.}
   \label{fig:dna}
\end{figure}

\section{The Central Dogma of Molecular Biology}
\label{sec:central_dogma}

In 1958, Francis Crick wrote a seminal paper on protein synthesis, where he described the importance of proteins in living organisms and first proposed the central dogma of molecular biology\cite{crick1958protein}. Crick described how DNA or ribonucleic acid (RNA) could be used as templates for proteins and further described the possible directions of information flow between DNA, RNA, and protein. However, he noted that once information had been transferred from either DNA or RNA to protein, it was not possible for information to flow back to nucleic acids (Figure ~\ref{fig:central_dogma}). In 1970, an enzyme known as reverse transcriptase (RT) was discovered\cite{pmid4316301,pmid4316300}, which allowed RNA to be used as a template for producing DNA. In light of this and due to the misunderstanding of the central dogma, Crick restated the central dogma\cite{CRICK1970}: ``The central dogma of molecular biology deals with the detailed residue-by-residue transfer of sequential information. It states that such information cannot be transferred from protein to either protein or nucleic acid."

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=1520,natheight=722,totalheight=0.20\textheight,keepaspectratio]{central_dogma.png}
   \caption[The central dogma]{All possible information transfer pathways between DNA, RNA, and protein are shown on the left. Probable (solid arrows) and possible (dotted arrows) information transfer pathways, as originally proposed in 1958 by Francis Crick\cite{crick1958protein}, are shown on the right. Note that once information has been transferred to a protein, it is not possible for this information to be transferred back to nucleic acids, which is known as the central dogma of molecular biology.}
   \label{fig:central_dogma}
\end{figure}

Prior to proposing the central dogma, Crick had predicted the existence of ``adaptors" that would transfer information from RNA to protein in 1955\cite{cricktrna1955}. Crick proposed that there were twenty adaptors and special enzymes, one for each amino acid; the enzymes would join a particular amino acid to its own special adaptor. This theory was later confirmed by the discovery of transfer RNA (tRNA) in 1958\cite{pmid13538965}. The discovery of messenger RNA (mRNA) in 1961 \cite{BRENNER1961} demonstrated the flow of information from DNA to RNA and from RNA to protein. In 1962, the DNA code used for encode the amino acids of proteins was deduced\cite{pmid14471390}. Matthaei and colleagues demonstrated that an artificially created RNA, composed entirely of uracils, would produce a protein composed entirely of phenylalanine. The full code, known as the genetic code, was cracked three years later in 1965\cite{pmid5330357} and defined how information was encoded in DNA to produce amino acids. Nirenberg and colleagues deduced that three nucleotides defined a codon, which are translated into one of the 20 standard amino acids.

\section{Transcription}

Transcription is the process by which a particular segment of DNA is processed into RNA by the enzyme RNA polymerase (RNA pol). There are three different types of RNA polymerases in eukaryotic cells: Pol I transcribes DNA that encode most of the ribosomal RNAs (rRNAs); Pol II transcribes DNA that encode mRNAs and other non-coding RNAs; and Pol III transcribes the genes for small regulatory RNA molecules, such as tRNAs. The first step in transcription is initiation, whereby RNA pol binds upstream of the DNA to be transcribed, at a region known as the promoter (Figure ~\ref{fig:transcription}). Promoters can be classified by their distance from the transcription start site (TSS), which are the first nucleotides transcribed by RNA pol. The core promoter for a region to be transcribed, i.e. the transcript, by Pol II is usually found immediately upstream of the TSS and contains specific DNA sequences or elements that are necessary for transcription. The core promoter elements include the TATA box (usually located 25 to 35 bases upstream of the TSS), the TFIIB recognition element [also known as the B recognition element, (BRE)], the initiator element (Inr), the downstream promoter element (DPE), and CpG islands (CGIs) (Figure ~\ref{fig:core_promoter}). The proximal promoter lies $\sim~250$ bp of the TSS and contains primary regulatory elements. Distal promoters do not have a fixed distance from the TSS but are usually further upstream and contain additional regulatory elements.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=626,natheight=216]{core_promoter.jpg}
   \caption[Core promoter elements]{Core promoter elements recognised by Pol II include the TFIIB recognition element and the TATA box, which are located around 35 and 25 bp upstream of the transcription starting site (TSS), respectively. Regulatory elements lie further upstream of the TSS. Image used with permission from Nature Education 2014.}
   \label{fig:core_promoter}
\end{figure}

Once transcription has initiated, RNA pol and its associated proteins unwind the DNA double helix; once unwound, RNA pol reads the template DNA strand and adds nucleotides to the 3' end of a nascent RNA transcript. Transcription is terminated when the RNA polymerase reaches the termination site and the mRNA transcript and RNA pol are released (Figure ~\ref{fig:transcription}). Transcription results in two main classes of RNA transcripts: (1) Protein-coding transcripts, where the RNA known as mRNA can be further translated into a protein molecule and (2) Non-coding transcripts, where the RNA molecule is the functional product.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=842,natheight=595,totalheight=0.70\textheight,keepaspectratio]{transcription.jpg}
   \caption[DNA transcription]{The process of transcription can be broadly grouped into three stages: a) initiation, b) elongation, and c) termination. Initiation involves the binding of RNA polymerase (shown as a large green blob) to the promoter and the DNA double helix starts to separate. RNA polymerase starts reading the sequence on the template strand in the 5' to 3' direction (green arrow). The elongation step involves the movement of RNA polymerase along the DNA strand producing a growing RNA transcript chain, which continually closes and opens the DNA strand. The nucleotides are shown as pink T-shaped molecules and the red arrow indicates that they are added at the 3' end of the nascent transcript. Once the RNA polymerase reaches the termination site, the RNA transcript and RNA polymerase are separated from the DNA. Image used with permission from Nature Education 2013.}
   \label{fig:transcription}
\end{figure}

\subsection{Transcriptional regulation}

The regulation of transcription ensures that transcripts are expressed in the correct spatial and temporal manner; this is necessary for maintaining cellular identity and for responding appropriately to environmental cues. Transcriptional regulation is achieved mainly through the interaction of proteins called transcription factors (TFs) and through the structural packaging of DNA. TFs are regulatory proteins that can activate or enhance the transcription of DNA by binding to specific DNA sequences and recruiting RNA polymerase\cite{pmid11092823}. TFs contain DNA-binding domains (DBDs) that enable it to bind specifically to DNA regions; these sites are known as transcription factor binding site (TFBS). One particular group of regulatory DNA sequence that TFs bind to are enhancer sequences, which when bound to leads to an enhancement in the rate of transcription. Enhancer sequences can be located thousands of nucleotides away from the promoter they interact with, as they are brought into proximity to the promoter by the physical looping of DNA. In addition, enhancers may be positioned in both forward and reverse orientations, and located either upstream or downstream from its associated promoter and still affect transcription.

The structural compaction of DNA into chromosomes limits the accessibility of DNA to TFs and RNA pol. This compaction is achieved mainly via histones, which are a family of small and positively charged proteins that fold negatively charged DNA in the form of electrostatic interactions; this folding helps condense DNA and the resulting DNA-histone complex is called chromatin. Chromatin possesses a fundamental repeating structure\cite{holde01111974}, known as the nucleosome, which is the structural and functional unit of chromatin. Nucleosomes are structured with two of each of the following histones: H2A, H2B, H3, and H4, and forms a histone octamer that binds and wraps about 146 base pairs of DNA. The H1 histone protein binds to DNA that links nucleosomes, called linker DNA, wrapping another 20 bps of DNA and stabilising the linker DNA. Chromatin is found in two varieties: heterochromatin, which features DNA tightly wrapped into a 30 nm fibre, and euchromatin where DNA is lightly packed as nucleosomes (Figure ~\ref{fig:dna_condensed}). 

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=627,natheight=614]{dna_condensed.jpg}
   \caption[DNA packaging]{DNA is condensed into chromosomes by forming DNA-protein complexes known chromatin, which is further coiled into thicker fibers called 30nm fibers. The chromosomes reside inside the nucleus of a cell; however, it should be noted that even when chromosomes are not condensed, such as during interphase, there is still the presence of condensed chromatin in the nucleus. Mitochondria contain their own DNA. Image used with permission from Nature Education 2013.}
   \label{fig:dna_condensed}
\end{figure}

\subsection{DNA accessibility}

Chromatin structure and nucleosome positioning are altered in order for the transcriptional and replication machinery to be able to access parts of the genome for transcription. Chromatin structure can be relaxed by biochemically modifying histones, to strengthen or weaken its association with DNA. Generally speaking, there are two major mechanisms by which chromatin is made more accessible via histone modifications:

\begin{enumerate}
   \item Histones can be enzymatically modified by the addition of acetyl, methyl, or phosphate groups.
   \item Histones can be displaced by chromatin remodelling complexes, thereby exposing underlying DNA sequences to polymerases and other enzymes.
\end{enumerate}

Importantly, these two processes are reversible, so modified or remodelled chromatin can be returned to its compact state after transcription and/or replication are complete. The nomenclature for histone modifications is defined by the name of the histone, followed by the single-letter amino acid abbreviation and its position, and then an abbreviation of the enzymatic modification; for example, H3K27ac indicates the acetylation of lysine 27 on H3. Specific histone modifications are associated with different biological states; for example, acetylation removes the positive charge on histones, thereby decreasing the interaction between histones and DNA, loosening chromatin, and leading to transcriptional activation. On the other hand, the tri-methylation of lysine 27 on histone H3, i.e. H3K27me3, is associated with the inhibition of transcription\cite{pmid21652639}. Given that distinct histone modifications can either activate or repress transcription, a ``histone code" has been proposed\cite{pmid11498575} and the profiling of the histone states provides insights into the transcriptional state of a DNA region. Table \ref{table:histone_mod} summarises a list of histone modifications and variants profiled by the ENCODE project\cite{pmid22955616}.

\begin{table}[h]
   \footnotesize
   \begin{tabular}{l l p{5cm}}
Histone modification or variant & Signal characteristics & Putative functions                                                                                                                    \\
H2A.Z                           & Peak                   & Histone protein variant (H2A.Z) associated with regulatory elements with dynamic chromatin                                            \\
H3K4me1                         & Peak/region            & Mark of regulatory elements associated with enhancers and other distal elements, but also enriched downstream of transcription starts \\
H3K4me2                         & Peak                   & Mark of regulatory elements associated with promoters and enhancers                                                                   \\
H3K4me3                         & Peak                   & Mark of regulatory elements primarily associated with promoters/transcription starts                                                  \\
H3K9ac                          & Peak                   & Mark of active regulatory elements with preference for promoters                                                                      \\
H3K9me1                         & Region                 & Preference for the 5′ end of genes                                                                                                    \\
H3K9me3                         & Peak/region            & Repressive mark associated with constitutive heterochromatin and repetitive elements                                                  \\
H3K27ac                         & Peak                   & Mark of active regulatory elements; may distinguish active enhancers and promoters from their inactive counterparts                   \\
H3K27me3                        & Region                 & Repressive mark established by polycomb complex activity associated with repressive domains and silent developmental genes            \\
H3K36me3                        & Region                 & Elongation mark associated with transcribed portions of genes, with preference for 3′ regions after intron 1                          \\
H3K79me2                        & Region                 & Transcription-associated mark, with preference for 5′ end of genes                                                                    \\
H4K20me1                        & Region                 & Preference for 5′ end of genes                                                                                                       
   \end{tabular}
   \caption[Table of histone modifications]{Summary of histone modifications and variants profiled by the ENCODE project\cite{pmid22955616}.}
   \label{table:histone_mod}
\end{table}

\section{DNA sequencing}

DNA sequencing is the process of determining the exact order of nucleotides within a DNA molecule. The first generation of DNA sequencing methods (Sanger and Maxam-Gilbert sequencing) were developed in the 1970s and were very labour intensive, requiring four separate polyacrylamide gel electrophoresis (PAGE) runs, for the determining the sequence of each base. The key feature of Sanger sequencing \cite{pmid271968} was the use of chain-terminating dideoxynucleotide triphosphates (ddNTPs). The structure of a normal nucleotide (dNTP), consists of a 3' hydroxyl (OH) group in the pentose sugar; chain-terminating ddNTPs lack the OH group that is necessary for the formation of the phosphodiester bond between one nucleotide and the next during DNA strand elongation. The idea was to set up a reaction with a mixture of dNTPs [deoxyadenosine triphosphate (dATP), deoxyguanosine triphosphate (dGTP), deoxycytidine triphosphate (dCTP), deoxythymidine triphosphate (dTTP)] and a particular ddNTP in a ratio of 300:1. Most of the times, the DNA will be elongated but if a ddNTP is incorporated into the growing DNA strand, strand elongation is terminated. This results in DNA fragments of varying lengths, where the last base of these fragments corresponding to the ddNTP used. By performing the same reaction for the other three ddNTPs and loading the fragments of each reaction onto separate PAGE lanes, the DNA bases can be deduced by reading the four lanes (Figure ~\ref{fig:sanger_ladder}).

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=160,natheight=332,totalheight=0.20\textheight,keepaspectratio]{sanger_ladder.jpg}
   \caption[Radioactively labelled sequencing gel]{PAGE, which has a 1 bp resolution, is used to separate the radioactively labelled DNA fragments from each reaction using specific chain-terminating ddNTPs. By reading the DNA fragments, the sequence of the DNA can be deduced. Image used under the terms and agreement of the Wikipedia GFDL.}
   \label{fig:sanger_ladder}
\end{figure}

The Maxam-Gilbert sequencing method\cite{pmid265521} relies on the use of chemicals that can cleave specific bases in contrast to chain-terminating ddNTPs. Dimethyl sulfate was used to cleave purine bases (A and G) and hydrazine was used to cleave pyrimidine bases (C and T). To distinguish the purines, an adenine-enhanced cleavage step is carried out, which cleaves adenines preferentially. To distinguish the pyrimidines, NaCl is used with hydrazine to suppress the reaction of thymines. As with Sanger sequencing, the DNA fragments are separated using PAGE, and the DNA bases are deduced by reading the gel.

Sanger sequencing became the \textit{de facto} method for DNA sequencing due to its comparative ease and the use of fewer toxic materials than Maxam-Gilbert sequencing. A further improvement to Sanger sequencing replaced the need to radioactively label the DNA fragments by using chemically synthesised fluorescent oligonucleotide primers\cite{pmid3713851}. Four different fluorophores were used for each ddNTP reaction allowing all four reactions to be co-electrophoresed and the DNA sequence was deduced by reading the fluorescence colours (Figure ~\ref{fig:sanger_sequencing}). The development of a fluorescence detection apparatus linked to a computer that processed the data created the world's first partially automated DNA sequencer\cite{pmid3713851}; this development was key towards the success of the Human Genome Project (HGP). For over 25 years since its inception, Sanger sequencing was the method of choice for DNA sequencing.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=600,natheight=447]{sanger_sequencing.jpg}
   \caption[Sanger sequencing]{a) DNA polymerase synthesises a complementary strand of DNA, however when a b) fluorescently labelled chain-terminating ddNTP base is incorporated, synthesis terminates producing DNA fragments of various sizes. c) As each terminator are fluorescently labelled with different dyes, each fragement will fluoresce a particular colour and the d) sequence trace is read by a computer that determines the sequence based on the coloured peaks.}
   \label{fig:sanger_sequencing}
\end{figure}

\subsection{Next-generation sequencing}

The next wave of DNA sequencing techniques, the so-called next-generation (next-gen) or second generation sequencing, started with various strategies that relied on a combination of template preparation, sequencing, and imaging that allowed thousands to billions of sequencing reactions to be performed simultaneo-usly\cite{pmid19997069}. Next-gen sequencing relies on the clonal amplification of templates and uses \textit{in vitro} cloning rather than bacterial cloning; the two most common methods of clonal amplification are emulsion polymerase chain reaction (emPCR)\cite{pmid12857956} and solid-phase amplification\cite{pmid16473845}. With emPCR individual DNA molecules are isolated with primer-coated beads in water-in-oil microreactors and clonal amplification leads to thousands of copies of the DNA molecule in an emulsion. 454 pyrosequencing and Sequencing by Oligonucleotide Ligation and Detection (SOLiD) sequencing employ emPCR and the amplification products are deposited into individual wells for sequencing. Solid-phase amplification relies on a lawn of high-density primers that are covalently attached on a slide surface (also known as a flow cell) and bind to DNA molecules that have been ligated with sequencing adaptors. The two methods allow each DNA template to be spatially separated and allow massively parallel sequencing to take place.

Sequencing can take place via the use of DNA polymerase, which is commonly known as sequencing-by-synthesis (SBS), or via the use of DNA ligase, which is known as sequencing-by-ligation (SBL). SBS can be further classified into cyclic reversible termination (CRT), single-nucleotide addition (SNA), and real-time sequencing\cite{pmid19997069}. CRT uses reversible terminators and initial developments used the dideoxynucleotides chain terminators used in Sanger sequencing. The concept of CRT is that a DNA polymerase incorporates one fluorescently modified nucleotide, which has a reversible terminator that terminates DNA synthesis. Unincorporated nucleotides are washed away and fluorescence imaging takes place to determine the identity of the incorporated nucleotide. The last step removes or cleaves the reversible terminator and the fluorescent dye, and the cycle is repeated. The CRT method is used in Solexa/Illumina and Helicos single-molecule fluorescent sequencing. SBL relies on DNA ligase and uses either one-base-encoded or two-base-encoded probes that are fluorescently labelled. The probes hybridise to its complementary sequence on the primed template and DNA ligase is added to join the probe to the primer. Non-ligated probes are washed away followed by fluorescence imaging and cleavage of the fluorescent dye and the cycle is repeated. The SBL method is used in SOLiD sequencing.

\subsection{Third generation sequencing and beyond}

The third generation of sequencing refers to single-molecule sequencing technologies, which has the capacity for generating longer read lengths at potentially cheaper costs\cite{pmid20858600}. One of the major advantages of single-molecule sequencing is that polymerase chain reaction (PCR) is not required, and therefore amplification biases and PCR mutations are eliminated. Furthermore, by employing third generation sequencing, quantitative applications of sequencing, such as RNA sequencing, can give a much more representative picture of the true abundance of RNA molecules. The HeliScope sequencer was the first commercially available single-molecule sequencer, which was based on the work of Stephen Quake and colleagues\cite{pmid12651960}. HeliScope sequencing utilises billions of primed single-molecule templates that are covalently attached to a solid support and uses CRT but with slight differences from Solexa/Illumina sequencing. HeliScope sequencing uses Helicos Virtual Terminators, which differ from the reversible terminators used in Solexa/Illumina sequencing and dye labelled nucleotides are added individually in the predetermined order of C, T, A, and G, followed by fluorescence imaging.

With the advent of high-throughput sequencing we now have the capacity to sequence an entire human genome in a matter of days. In addition, we have just recently arrived in the \$1,000 genome era, whereby we can sequence the entire genome of an individual at a 30x depth (the minimum depth required for clinical applications) for around \$1,000 US dollars (USD). In contrast, the Human Genome Project (HGP), which gave us the first glimpse of the human genome\cite{lander2001initial} costed approximately 2.7 billion fiscal year 1991 US dollars\cite{nhgri2010cost}. Further developments in sequencing by various companies are aiming towards longer read lengths at a higher output (Figure ~\ref{fig:dev_next_gen}). Currently, different sequencers either have very long reads but at a low-throughput or have a high-throughput of shorter reads; as such, each sequencer fills a particular niche. \textit{De novo} assembly of genomes requires longer reads for less ambiguity and the quantification of RNA requires higher throughput in order to accurately sample the vast RNA population.

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=720,natheight=540]{developments-in-next-generation-sequencing.jpg}
   \caption[Developments in next generation sequencing]{Log read length versus log gigabases per run for various high-throughput sequencer\cite{Nederbragt2012}. Currently, the HiSeq X, the sequencer that bought us into the \$1,000 genome era, provides the highest throughput.}
   \label{fig:dev_next_gen}
\end{figure}

\section{Expression analysis}

Transcription of a region of DNA results in the expression of an RNA transcript. A transcript may be constitutively expressed, i.e constantly expressed, or expressed according to the current cellular requirements. By comparing transcript levels between different conditions, insight can be gained on the possible function of a particular transcript. Of note is that the amount of a specific transcript in a cell at a given time is not only influenced by the rate of transcription but also by the stability of the transcript; a rapidly degraded transcript may appear to be lowly transcribed. Northern blotting\cite{pmid414220} was one of the first methods for quantifying the expression level of specific RNA transcripts. This technique involves the electrophoretic separation of purified RNA, followed by immobilising the RNA onto a blotting membrane; detection of the transcript is achieved by hybridising a specific probe that is complementary to part of the transcript. The relative amount of a specific transcript can be estimated by comparing the strength of signals from different samples; this estimate assumes that an equivalent amount of total RNA was used per sample and this is verified by using a probe that detects a constitutively expressed transcript. Northern blotting is also useful for determining the size of a specific transcript and is commonly used for detecting alternatively spliced transcripts; however, Northern blotting is not very sensitive for estimating transcript abundance.

The most sensitive method of detecting specific transcript levels is quantitative real-time polymerase chain reaction (qRT-PCR), which is able to detect sparse levels of transcripts, down to the level present in single cell. The method employs the use of a fluorescent dye and PCR, where the primer pairs are designed to be specific for a given transcript. By measuring the number of cycles that is required for the formation of a detectable amount of product, which is known as the cycle threshold ($C_{t}$) value, the transcript levels can be estimated by comparing the results to qRT-PCR experiments performed using standard samples; lower $C_{t}$ values indicate higher amounts of initial template. It should be ensured that the PCR step is equally efficient for different transcripts by proper primer design, as a more efficient PCR will result in a lower $C_{t}$ value compared to a less efficient PCR.

\subsection{Transcriptome profiling}

Transcriptome profiling refers to the expression profiling of the complete collection of transcripts in a cell at a specific time point. Transcripts associated with a particular biological process or disease state can be identified by profiling all transcripts and in addition, the interplay between transcripts can be inferred by studying transcripts with similar expression patterns. One of the first technologies that allowed the simultaneous profiling of thousands of transcripts at once were microarrays\cite{pmid7569999}. DNA probes that are complementary to specific DNA sequences, such as complementary DNAs (cDNA) or genomic regions, are attached to a solid surface and fluorescently labelled target sequences are hybridised onto the surface. Target sequences that complement the probe sequences hybridise to the probe and the signal intensity provides a measure of the expression strength of a particular transcript. In one of the first application of microarrays, researchers were able to observe the change in expression levels of 700 mRNAs during a switch from aerobic to anaerobic respiration in yeast cells\cite{pmid9381177}. However, microarrays have several limitations, which includes requiring \textit{a priori} knowledge of the genome or transcript sequences, high background levels from cross-hybridisation\cite{pmid16749918}, and a limited dynamic range in quantifying expression.

In contrast to the hybridisation approach of microarrays, sequencing-based approaches have been developed for transcriptome profiling. Prior to the advent of next-gen sequencing, sequencing approaches were based on the sequencing of short tags; these tagging approaches were cost-effective, as only short fragments of cDNAs were sequenced. Typically type IIS restriction enzymes were used to create tags, which were then concatenated, cloned, and sequenced. A technology called Serial Analysis of Gene Expression (SAGE)\cite{pmid7570003} was the first tag-based approach, which created 9 to 10 bp long tags that generally corresponded to the 3' end of the transcripts. A similar technology known as Massive Parallel Signature Sequencing (MPSS), was later developed, which is similar to SAGE but employs different biochemical steps and a different sequencing approach\cite{pmid10835600}. MPSS was an improvement to SAGE in that it produced longer tags (16-20 bp) and libraries that were 20 times larger than typical SAGE libraries\cite{pmid10835600}. Another tagging method known as the paired-end ditag (PET) approach, prepared ditags that corresponded to the 5’ and 3’ end of the same full-length cDNA\cite{pmid15782207}. The PET approach allows the mapping of cDNA boundaries, helps resolve ambiguous tag mappings by using the paired-end information, and has the potential to detect unconventional fusion transcripts and rearrangement events. The SAGE approach also gave rise to Cap Analysis Gene Expression (CAGE)\cite{pmid14663149}, which combines the tagging strategies of SAGE with a molecular technique known as Cap-Trapper\cite{pmid8938445,pmid9179497}. The CAGE protocol captures all capped transcripts and sequences a short tag (20 or 27 nt depending on which restriction enzyme is used) that corresponds to the 5' end of an RNA transcript (Figure ~\ref{fig:cage_protocol}).

\begin{figure}[!ht]
   \centering
   \includegraphics[width=\textwidth,natwidth=2254,natheight=3051,totalheight=0.65\textheight,keepaspectratio]{cage_protocol.png}
   \caption[Cap Analysis Gene Expression protocol]{The Cap Analysis Gene Expression (CAGE) protocol starts with synthesising cDNA from either total RNA or mRNA by using random or oligo dT primers (only random primers are shown here). Reverse transcription takes place in RNAs with or without a cap and to full or partial completion; the RNase I digestion removes partially reverse transcribed RNA as they are not protected by a full double strand. The 5' end of cDNAs are selected by streptavidin beads and unbound cDNA are washed out. After release from the bead, a linker is attached to the 5' end of the single-stranded cDNA; this linker contains recognition sites that allow the endonuclease cleavage. Lastly a linker is attached to the 3' end of the tag sequence, which is amplified and directly sequenced.}
   \label{fig:cage_protocol}
\end{figure}

With the arrival of next-gen sequencing, the SAGE and CAGE methods were adapted to high-throughput sequencers\cite{pmid14676315,Takahashi2012}. The short-read and high-throughput nature of next-gen sequencing suited tag-based approaches aptly, as the tag lengths were in the size range of reads produced by the sequencer. Whole transcriptome shotgun sequencing or simply RNA sequencing (RNA-Seq) methods were later developed to sequence entire populations of RNA. RNA-Seq refers to the fragmentation of RNA followed by deep-sequencing on next-gen sequencing platforms\cite{pmid19015660}; the fragmentation step is required due to the short-read nature of next-gen sequencers. Typically, RNA-Seq methods enrich for transcripts with a poly-A tail; this enrichment step is carried out to avoid rRNA sequences, which make up a large fraction of the total RNA population. One major difference between RNA-Seq and the tag-based approaches is that the entire length of the RNA is sequenced in RNA-Seq; the advantage in this is that alternative splicing patterns can be inferred. However tag-based approaches, such as CAGE, can be used in a complementary nature to RNA-Seq\cite{pmid24676093}, since transcript boundaries are defined clearly in CAGE.

\subsection{Transcriptional complexity}

Several landmark studies have revealed that the transcriptional landscape is much more complex than previously anticipated. The functional annotation of the mammalian genome (FANTOM) project, which began as an initiative to sequence and functionally annotate mouse full-length complementary DNA (cDNA)\cite{pmid11217851}, revealed that the transcriptome was dominated by transcripts that had no apparent coding potential\cite{pmid12466851}. The FANTOM consortium also revealed massive antisense transcription\cite{pmid16141073}, which is transcription arising from the strand opposite the sense strand, and extensive alternative promoter usage\cite{pmid16141072}. Tiling arrays, which are microarrays designed to interrogate a genome at evenly spaced intervals, revealed that a large fraction of genomic bases were transcribed\cite{pmid15539566, pmid11988577, pmid15790807}. This observation that a large percentage of mammalian genomes are transcribed became known as pervasive transcription\cite{pmid17510325}, however these claims made on the basis of tiling arrays were questioned\cite{pmid20502517}. However, the Encyclopaedia of DNA elements (ENCODE) project, which endeavours to identify all functional elements in the human genome, observed that mammalian genomes were pervasively transcribed\cite{pmid17571346, pmid22955616}. These claims were based on the use of various genome-wide biochemical assays and multiple lines of evidence. However, it is not known whether these products of transcription are functional or not, as they do not overlap known genes.

\subsection{Defining a gene}

The idea of a gene dates back to Gregor Mendel and his plant breeding experiments that demonstrated that discrete traits could be inherited from parents to offspring. The term ``gene" was coined in 1909 by Wilhelm Johannsen to describe the Mendelian units of heredity. Genes were later described as the precursors to proteins, in the ``one gene, one polypeptide" hypothesis, when it was observed that mutations in Neurospora genes would cause defects in different steps of metabolic pathways \cite{Beadle15111941}. After the determination of the genetic code (see section~\ref{sec:central_dogma}), a gene was recognised as a stretch of DNA that coded for a protein in an open reading frame (ORF). The discovery of introns\cite{pmid890740,pmid922889}, altered the ORF concept, in that genes were now composed of both protein-coding regions (exons) and the non-coding regions (introns). The trend had been that each time a major discovery had been made, the definition of a gene was revised. Thus in light of pervasive transcription members of the ENCODE consortium suggested that the definition of a gene needs to be updated yet again\cite{pmid17567988}. The definition they proposed was: ``A gene is a union of genomic sequences encoding a coherent set of potentially overlapping functional products"\cite{pmid17567988}. This definition is similar to the idea of a transcriptional unit (TU), proposed by the FANTOM consortium, which is a segment of the genome that shares common core of genetic information and is able to generate transcripts\cite{pmid12466851}.

\subsection{Non-coding RNAs}

The protein-centric view of genes segregated RNA as either being coding RNA, the mRNAs,  or RNA with no coding potential, known as non-coding RNAs (ncRNAs). Historically, it was believed that there were only a few ncRNA families, such as the tRNAs, rRNAs, and the spliceosomal RNAs, all of which existed to aid protein translation. NcRNAs exist as single-strand nucleic acid molecules, which tend to fold on itself due to the hydrophobic nature of bases, to form localised double-stranded regions that form structures called hairpins or stem-loop structures. NcRNAs can be broadly classified as short or small RNAs, which are defined as being \textless200 nucleotides long, or as long non-coding RNAs (lncRNAs), which are \textgreater200 nucleotides in length; these size cut-offs corresponds to the commonly used size selections in biochemical fractionations.

One of the most well studied classes are the micro-RNAs (miRNAs), which were first observed in \textit{Caenorhabditis elegans}\cite{pmid8252621}. MiRNAs are typically 20-24 nucleotides in length and functions as regulators of expression by base-pairing with complementary sequences within mRNAs (commonly to the 3' untranslated region (UTR)). Another class of well known ncRNAs are the Piwi-interacting RNA (piRNA), which were first observed in drosophila\cite{pmid11470406}. PiRNA are typically between 24 and 32 nucleotides long and are thought to be involved in gene silencing, especially in silencing transposable elements (TEs), by forming the piRNA-induced silencing complex (piRISC). The characterisation of full-length cDNAs revealed another class of RNAs known as long non-coding RNAs (lncRNAs)\cite{pmid12466851}. Unlike, miRNAs and piRNAs, they are not as well characterised and are usually described with respect to protein-coding genes as sense, antisense, bidirectional, intronic, and intergenic\cite{pmid19239885}. LncRNAs have been dismissed due to lack of sequence conservation\cite{pmid15495343} and have been suggested to be products of transcriptional noise\cite{pmid15851066}. While there have been several well documented cases of lncRNAs, it has been suggested that more experimental work needs to be performed on lncRNAs to find out how many are indeed functional\cite{pmid23463798}.

The list of ncRNAs has increased over the last 20 years due to technological advances, such as RNA-Seq. These include, but are not limited to, promoter upstream transcripts\cite{pmid19056938}, long intergenic non-coding RNAs\cite{pmid21890647, pmid22196729, pmid2943744}, transcription start site-associated RNA\cite{pmid21822281}, enhancer RNAs\cite{pmid20393465}, promoter-associated short RNAs and termini-associated short RNAs\cite{pmid17510325}, double strand break-induced small RNAs\cite{pmid22445173} or DNA damage RNAs\cite{francia2012site}, competing endogenous RNAs\cite{pmid24429633}, and transcription initiation RNAs\cite{pmid19377478}. Despite an abundance of ncRNA classes, many of these ncRNAs are supported only by transcriptional data, since validations studies have only been performed in a low-throughput manner. As the genome is pervasively transcribed, transcription by itself is not enough evidence to support function. Newer technologies, such as parallel analysis of RNA structure\cite{pmid20811459} and fragmentation sequencing\cite{pmid21057495}, that focus on the structural properties of RNA may provide more clues to the extent of functionality of ncRNAs.

\section{Repetitive mammalian genomes}

The discovery that cells contained a large fraction of repetitive DNA was made by measuring the re-association rates of DNA strands after denaturation\cite{Britten1968}. The characterisation of repetitive elements (REs) was possible with the release of the mouse\cite{pmid12466850} and human\cite{venter2001sequence, lander2001initial} genome sequences, which showed that these genomes are indeed largely made up of REs. The two major groups of REs are tandem repeats, which include different classes of satellite repeats, and interspersed repeats, which are mostly made up of transposable elements (TEs)\cite{pmid9666329}. Within TEs are two main classes: Class I TEs or retrotransposons, which are DNA elements that are transcribed into RNA, reverse transcribed back to DNA, and transposed to a new location in the genome (a copy-and-paste mechanism), and Class II TEs or DNA transposons, which simply excise their DNA sequence from one location to another via transposase enzymes (a cut-and-paste mechanism). As retrotransposons are able to produce a copy of themselves before propagation, they are more numerous than DNA transposons. The retrotransposon known as the Alu element, has an estimated copy number of more than one million, making them the most abundant RE in the human genome\cite{lander2001initial}.

There are various methods for identifying REs in genomes, which can be broadly categorised into \textit{de novo}, homology, structure, and comparative genomic based methods\cite{Bergman01112007}. For the initial mouse and human genome sequencing projects, REs were catalogued using a popular software called RepeatMasker, which identifies REs by using homology-based methods to search against a database of consensus repeats\cite{pmid19274634}, such as the Repbase Update database\cite{pmid16093699}. One of the main drawbacks of identifying REs in this manner is the reliance on sequence homology and a single consensus sequence, resulting in missing REs with an extensive number of mutations. Recently, RepeatMasker has incorporated the use of profile Hidden Markov Models to annotate REs\cite{pmid23203985}; profile methods use an alignment of multiple representative sequences rather than a single consensus and are more sensitive than single sequence searches. However, REs not contained within databases may still be missed and \textit{de novo} methods may be the key to identifying these repeats. It has been proposed that up to two-thirds of the human genome may be made up of repetitive elements based on a \textit{de novo} identification method\cite{pmid22144907}.

\subsection{Junk DNA}

Historically, TEs have been labelled as purely selfish elements that have no function or provide no selective advantage to an organism\cite{doolittle1980selfish, orgel1980selfish}.

The origin of the term ``junk DNA" is usually attributed to Susumu Ohno\cite{pmid5065367}, who used it to describe pseudogenes, which are gene copies that have no known biological function. In its modern day usage, ``junk DNA" is used to describe DNA sequence that does not play a functional role in an organism. Junk DNA has recently been bought into the spotlight by the ENCODE project, which reported that 80\% of the human genome has a biochemical function\cite{pmid22955616} and was translated as an eulogy for junk DNA\cite{Pennisi07092012}. ENCODE's reported findings were criticised since having biochemical activity alone is insufficient for claiming function\cite{pmid23431001, pmid23479647, Eddy2012}. Furthermore, in terms of mutational load it is impossible that 80\% of the human genome is functional as this would lead to mutational meltdown\cite{pmid24809441}. The idea, which was pointed out by Susumu Ohno\cite{pmid5065367}, was that given a fixed mutation rate (each locus has a $10^{-5}$ probability of sustaining a deleterious mutation), the number of functional loci in the human genome must reach a limit due to genetic load. He predicted that mammalian genomes could not have more than 30,000 loci under selection as this would guarantee a progressive decline in genetic fitness, leading to extinction. However, the contrary, that 80\% of the ``functional" sites reported by ENCODE are non-functional is probably not true. Based on sequence conservation, it seems that around 5-20\% of the human genome is under detectable selective pressure\cite{Eddy2012}. As for the question of why there is so much junk DNA in the human genome, Sydney Brenner gives us his take on the subject in his Nobel lecture\cite{brennernobellecture}:

\subsection{Impact of repetitive elements on genomes}

However, the authors did leave open the possibility that some TEs may become useful:

\epigraph{``It would be surprising if the host genome did not occasionally find some use for particular selfish DNA sequences, especially if there were many different sequences widely distributed over the chromosomes. One obvious use $\ldots$ would be for control purposes at one level or another. This seems more than plausible."}{--- \textup{Orgel and Crick 1980}}

Indeed there is an increasing appreciation of TEs serving as a source for evolutionary innovation\cite{Muotri15102007, pmid18368054}. TEs may become exapted, a process where TE have acquired function, which may be positively selected for. For example, TEs may acquire a regulatory function by providing TFBSs for TFs and thereby drive the expression of a nearby DNA. In a study examining the binding sites of various TFs, it was discovered that REs can provide binding sites for TFs; these sites were termed repeat-associated binding sites (RABS)\cite{pmid18682548}. Importantly, the study demonstrated that RABS are over-represented in proximity of regulated genes and that the binding motifs within these repeats have undergone evolutionary selection. CAGE also demonstrated that many TEs are indeed transcribed in a tissue-specific manner and served as alternative promoters\cite{pmid19377475}. Corroborating with tissue-specific expression of TEs was a study demonstrating that methylation patterns of TEs differed amongst different tissues\cite{pmid23708189}. Though not a definitive measure, tissue-specific expression patterns are still useful for separating signal from noise as expression patterns are not a consequence of random experimental noise.

Various criteria have been used as a criteria for functionality, in contrast to noise, in transcriptional products; these can be broadly separated into \textit{in silico} and experimental validations. \textit{In silico} methods are based on analysing properties associated with an expressed transcript. The expression strength is a commonly used criteria, as consistently captured signal is less likely to occur just by chance, i.e. the smaller the margin of error with an increase in signal. Context-specific transcription, such as observed expression at a specific time-point, developmental stage, or tissue type, can also be used to support function. The argument is such that a random transcript would not become active under a specific condition. However, the criteria of context-specific expression patterns can be countered by the fact that different cell types and during different developmental stages, express a specific repertoire of transcription factors and are under certain circumstances that would lead to the context-specific transcription. Sequence conservation is usually used to support functionality, as this is evidence for purifying or negative selection. For example, protein-coding genes are typically very well conserved among different organisms. Lastly, transcriptional data is typically shown in conjunction with other genome-wide assays.

Typically experiments are required for validating computational predictions. Transcript expressions are validated by qRT-PCR.

\begin{itemize}
   \item Experimental manipulation
   \item Observable phenotypic change with experimental manipulation
   \item Molecular assays
   \item Reporter-gene assays, such as placing putative regulatory sequences upstream of a reporter gene
   \item Tissue or intracellular localisation of transcripts can be determined using RNA FISH
   \item Biological assays
   \item assay how a genetic element produces a measurable phenotypic effect by the
   \item Use of cell lines to knockdown or transfect cells with a specific transcript
   \item Use of animal models to knock-out or knock-in transcripts
\end{itemize}

\section{Bioinformatics and genomics}

Modern day high-throughput sequencers generate a large amount of data and dedicated informatics tools for storing, managing, and the analysis of such data sets are absolutely necessary. Bioinformatics can be thought of as a subset of informatics that deals with biological data, though historically it was defined as ``the study of informatic processes in biotic systems"\cite{pmid21483479}. The HGP was one of the first large scale international research efforts, which demonstrated how bioinformatics was crucial towards the successful completion of the project\cite{stein1996perl}. Furthermore, the HGP also set the stage for data sharing by establishing important principles, known collectively as the ``Bermuda Principles", that promoted the rapid and public sharing of human genome information. These set of commitments left a lasting legacy in large genomic science projects such as The International HapMap Project, ENCODE and modENCODE, and The Cancer Genome Atlas, where data are made freely available prior to publication\cite{contreras2011bermuda}. By opening such resources, researchers are able to integrate and leverage these datasets for generating testable hypothesises.

While many of the foundations in bioinformatics were related to molecular evolution and population genetics, the availability of complete genome sequences has opened up an entire research discipline called genomics. The field of genomics is also closely tied with bioinformatics, as genomics studies typically deals with large amounts of data, corresponding to features of genomes. There are several sub-branches within genomics; the field of functional genomics focuses on the dynamic aspects of genomes, such as transcription, interactions between proteins and genomic regions, and DNA methylation patterns. Functional genomics, as the name suggests, attempts to discover and establish function to elements in the genome and usually employing the use of high-throughput methods for genome-wide screening.

\subsection{High-throughput sequencing data}

The FASTQ format was formally defined in 2010\cite{pmid20015970} and has become the \textit{de facto} format for storing raw high-throughput sequencing data. FASTQ is similar to the FASTA format but with the addition of quality scores, known as the Phred quality score, for each sequenced nucleotide and is usually the starting point of bioinformatic pipelines. Typically, quality control (QC) steps are carried out next to remove potential artefacts and low quality reads; one such tool known as TagDust\cite{pmid19737799}, removes reads that match sequences used during the library preparation, such as primer and adaptor sequences. Other QC steps include removing reads containing undetermined bases, as they indicate poor overall read quality; for sequencers that output reads of different lengths, reads outside the main length distribution are removed; quantifying highly over-represented 10-mers can also be implemented as a QC step\cite{pmid21088025}, to identify potential artefactual sequences.

In order to put sequencing reads into context, reads are mapped onto their corresponding reference genome; various tools are available for aligning high-throughput sequencing reads. Traditional tools such as BLAST\cite{pmid2231712} and BLAT\cite{pmid11932250} are unable to cope with the large quantity and short length of reads from high-throughput sequencers. One popular short-read alignment tool, BWA\cite{pmid19451168}, implements the Burrows–Wheeler transform to deal with millions to billions of short reads. The Burrows-Wheeler transform allows a large mammalian genome, for example human, to be indexed and stored efficiently into memory\cite{pmid19430453}. The Sequence Alignment/Map (SAM) format\cite{pmid19505943} is the standard file format for storing sequence alignments and contains all the information for reconstructing an alignment. In addition the SAM format contains information on where a read maps on the genome, the quality of the mapping, and depending on the alignment program, other mapping statistics. The open source program SAMTools\cite{Li15082009}, provides various utilities for the processing and analysis of alignments stored in the SAM format. In order to save disk space, SAM files are typically stored as BAM files, which are simply their binary equivalent. Recent developments have introduced a newer format known as CRAM, based on a newer compression method\cite{pmid21245279}, which further compresses BAM files but are still processable using SAMTools.

The BED format\cite{bedformat} is another standard file format used for storing the location of a set of features (or even sequencing reads) with respect to a reference genome and has been popularised by the UCSC Genome Browser\cite{Kent01062002}. Due to the popularity of the BED format, a suite of tools released as BEDTools\cite{pmid20110278}, provides various routines for comparing genomic features stored in BED format. A common task in processing RNA-Seq data involves intersecting mapped reads to known genomic features, such as genes, to associate a read to a gene and thereby quantifying its expression. In addition to this, the proximity of elements on a chromosome may indicate potential functional interactions, thus reads may be annotated with respect to the physical distance, i.e. spatially, to genomic features. For example, promoters are usually upstream and nearby the genes that it initiates transcription for, thus CAGE reads are associated with nearby gene models in this manner.

\subsection{Analysing expression datasets}

Expression data sets are typically represented as matrices; for example, if we let $A$ be an $m \times n$ matrix, where $a_{ij}$ are elements of $A$, then the $i^{th}$ row would represent the transcriptional response of the $i^{th}$ transcript and the $j^{th}$ column would represent the expression profile of the $j^{th}$ assay:

\begin{align*}
   A = \begin{bmatrix} a_{11} & \cdots & a_{1j} & \cdots & a_{1n} \\
   . && . && . \\
   a_{i1} & \cdots & a_{ij} & \cdots & a_{in} \\
   . && . && . \\
   a_{m1} & \cdots & a_{mj} & \cdots & a_{mn} \end{bmatrix}
\end{align*}

Typically, two or more groups of assays are produced, such as a set of control experiments versus a set of treated experiments, and the aim is to find differentially expressed transcripts between the two conditions. Data normalisation is required prior to comparing assays to ensure that experimental factors (not including the experimental treatment) that cause differences are accounted for. For example, a library that has been sequenced twice as much as another library will have most of its transcript as seemingly expressed twice as much. One way to account for this is to normalise reads by counts or tags per million (TPM); to normalise by TPM the $i^{th}$ gene in the $j^{th}$ assay:

\begin{align*}
   TPM_{a_{ij}} = \frac{a_{ij} \times 1000000}{\sum_{i=1}^{m}{a_{mj}}}
\end{align*}

Other methods include normalising the expression by the length of a transcript, known as reads per kilobase of transcript per million mapped reads or fragments per kilobase of transcript per million mapped reads, for RNA-Seq experiments where reads are generated throughout the length of the transcript, quantile normalization, which is a technique for making two distributions identical in statistical properties\cite{pmid12538238}, and trimmed mean of M values (TMM), which is a normalisation method that takes into account differences in the RNA population from different samples by scaling the expression\cite{pmid20196867}.

After the appropriate normalisation methods have been carried out, the testing of differential expression is performed by comparing the amount of variation between groups against the amount of variation within groups for a particular transcript, which is similar to carrying out a \textit{t}-test or analysis of variance (ANOVA). However, the simple assumptions of these tests are not met due to the heteroscedastic and non-normal distribution properties of high-throughput sequencing data. RNA-Seq expression is measured by digital counts of reads, meaning that expression levels are discrete, and the variance is modelled using discrete probability distributions. In a pioneering study examining the reproducibility of RNA-Seq, it was noted that the variation between technical replicates was close to the shot noise limit\cite{pmid18550803}. Thus it was suggested that the Poisson model was sufficient in modelling the variance and used for testing differential expression. However, it was demonstrated that the Poisson model underestimated the effects of biological variability, i.e. the variation between two different biological samples is greater than Poisson variation\cite{20979621}. This can be accounted for by modelling variance under a negative binomial model, which has been implemented for differential expression analysis in the edgeR package\cite{pmid19910308} from Bioconductor\cite{pmid15461798}.

The correlation of transcriptional responses or expression profiles can be calculated to identify transcripts expressed in a similar manner or assays with a similar profile, respectively. Correlation measures such as Pearson's product-moment correlation coefficient or Spearman's rank correlation coefficient are typically used and can be used as a measure of co-expression, i.e. transcripts with a similar transcriptional response are assumed to be co-expressed. Other measures of similarity or rather dissimilarity include metrics such as the Euclidean, maximum, Manhattan, Canberra, Jaccard, and Minkowski distances. Hierarchical clustering is usually performed in an agglomerative manner to reveal the topology of distance matrices and visualised using dendrograms to revealing the most similar transcripts or assays. The visualisation of expression datasets include heatmaps, which transforms the expression matrix into colours representing the relative expression strength and are commonly arranged according to the hierarchical clustering structure\cite{pmid9843981}. Graphs can also be used to represent associations of transcripts or assays, which can leverage methodologies developed in graph theory.

Expression datasets are usually large and therefore subjected to the multiple testing problem, whereby significant p-values are observed due to the large number of statistical inferences that are carried out. Several techniques are used to account for this, such as the Bonferroni correction or false discovery rate (FDR) control\cite{fdr_paper} and these techniques generally require a higher p-value significance threshold to compensate for the number of inferences made. A FDR of 10\% means that it is expected that 10\% of our statistical inferences are false positives and the FDR is the p-value at which to draw a threshold. It is important to account for multiple testing when performing numerous statistical tests, which is common when analysing high-throughput expression datasets.
